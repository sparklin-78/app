import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import json 

from sklearn.preprocessing import LabelEncoder 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.model_selection import train_test_split 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report 
import re # NLP
import nltk # natural langauge processing
from nltk.tokenize import word_tokenize 
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

from wordcloud import WordCloud 

import tensorflow as tf 
from tensorflow.keras import Sequential 
from tensorflow.keras.layers import Dense 
from tensorflow.keras.utils import plot_model 
from tensorflow.keras.callbacks import EarlyStopping 
from tensorflow.keras.models import load_model 

from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import RandomizedSearchCV 
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier 

%matplotlib inline 

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset.csv', encoding='utf-8').copy() 
                                                          
print(data.shape)  
data.head()
data.tail()

data['language'].unique() 

data.isnull().sum()
data.dtypes

data['language'].value_counts()

data = data.drop_duplicates(subset='Text')
data = data.reset_index(drop=True)
data['language'].value_counts()

import nltk
nltk.download('stopwords')
print(stopwords.fileids())

nonalphanumeric = ['\'', '.', ',', '\"', ':', ';', '!', '@', '#', '$', '%', '^', '&',
                 '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', '\\', '?', 
                 '/','>', '<', '|', ' '] 

stopwords = nonalphanumeric

len(stopwords)

def clean_text(text):
    tokens = word_tokenize(text) 
    words = [word.lower() for word in tokens if word not in stopwords] 
    words = [PorterStemmer().stem(word) for word in words] 
    return " ".join(words)

import nltk
nltk.download('punkt')
data['clean_text'] = data['Text'].apply(clean_text)

le = LabelEncoder()
data['language_encoded'] = le.fit_transform(data['language'])
data.head()

lang_list = [i for i in range(22)]
lang_list = le.inverse_transform(lang_list)
lang_list = lang_list.tolist()
lang_list

plt.figure(figsize=(10,10))
plt.title('Language Counts')
ax = sns.countplot(y=data['language'], data=data)
plt.show()

def remove_english(text):
    pat = "[a-zA-Z]+"
    text = re.sub(pat, "", text)
    return text

data_Chinese = data[data['language']=='Chinese'] 

clean_text = data.loc[data.language=='Chinese']['clean_text']
clean_text = clean_text.apply(remove_english) 

data_Chinese.loc[:,'clean_text'] = clean_text

data.drop(data[data['language']=='Chinese'].index, inplace=True, axis=0)
data = data.append(data_Chinese)

data =data.sample(frac=1).reset_index(drop=True)
x = data['clean_text']
cv = CountVectorizer()
x = cv.fit_transform(x)
x = x.astype('uint8')

y = data['language_encoded']

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)

NB_model = MultinomialNB()
NB_model.fit(x_train, y_train)

y_pred = NB_model.predict(x_test)
accuracy_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(12,10))
plt.title('Confusion Matrix - NB_model')
sns.heatmap(cm, xticklabels=lang_list, yticklabels=lang_list, cmap='rocket_r', linecolor='white', linewidth=.005)
plt.xlabel('Predicted Language', fontsize=15)
plt.ylabel('True Language', fontsize=15)
plt.show()

print(classification_report(y_test, y_pred))

x_train = x_train.toarray()
x_test = x_test.toarray()

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

INPUT_SIZE = x_train.shape[1]
INPUT_SIZE

OUTPUT_SIZE = len(data['language_encoded'].unique())
OUTPUT_SIZE

EPOCHS = 10
BATCH_SIZE = 128

model = Sequential([
    Dense(100, activation='relu', kernel_initializer='he_normal', input_shape=(INPUT_SIZE,)),
    Dense(80, activation='relu', kernel_initializer='he_normal'),
    Dense(50, activation='relu', kernel_initializer='he_normal'),
    Dense(OUTPUT_SIZE, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

hist = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.3, verbose=2)

model.summary()

plot_model(model, show_shapes=True)

plt.title('Learning Curve')
plt.xlabel('Epochs')
plt.ylabel('Categorical Crossentropy')
plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='val')
plt.legend()
plt.show()

plt.title('Learning Curve')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(hist.history['accuracy'], label='train')
plt.plot(hist.history['val_accuracy'], label='val')
plt.legend()
plt.show()

loss, accuracy = model.evaluate(x_test, y_test, verbose=2)
print('Accuracy %.3f'%accuracy)

x = data['clean_text']

cv = CountVectorizer() 
x = cv.fit_transform(x)
x = x.astype('uint8')

y = data['language_encoded']

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)

x_train = x_train.toarray()
x_test = x_test.toarray()

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

INPUT_SIZE = x_train.shape[1]
INPUT_SIZE

OUTPUT_SIZE = len(data['language_encoded'].unique())
OUTPUT_SIZE

BATCH_SIZE = 256
EPOCHS = 8

es = EarlyStopping(monitor='accuracy', patience=1)

model = Sequential([
    Dense(100, activation='softsign', kernel_initializer='glorot_uniform', input_shape=(INPUT_SIZE,)),
    Dense(80, activation='softsign', kernel_initializer='glorot_uniform'),
    Dense(50, activation='softsign', kernel_initializer='glorot_uniform'),
    Dense(OUTPUT_SIZE, activation='softmax')
])

model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

hist = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.3, callbacks=[es], verbose=2)

model.summary()

plot_model(model, show_shapes=True)

plt.title('Learning Curve')
plt.xlabel('Epochs')
plt.ylabel('Categorical Crossentropy')
plt.plot(hist.history['loss'], label='train')
plt.plot(hist.history['val_loss'], label='val')
plt.legend()
plt.show()

plt.title('Learning Curve')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(hist.history['accuracy'], label='train')
plt.plot(hist.history['val_accuracy'], label='val')
plt.legend()
plt.show()

loss, accuracy = model.evaluate(x_test, y_test, verbose=2)
print('Accuracy %.3f'%accuracy)

y_pred_prob = model.predict(x_test) 
y_pred = []
for i in y_pred_prob:
    out = np.argmax(i)
    y_pred.append(out)
y_pred = np.array(y_pred)

cm = confusion_matrix(y_test, y_pred) 

plt.figure(figsize=(12,10))
plt.title('Confusion Matrix - MLP Model')
sns.heatmap(cm, xticklabels=lang_list, yticklabels=lang_list, cmap='rocket_r', linecolor='white', linewidth=.005)
plt.xlabel('Predicted Language', fontsize=15)
plt.ylabel('True Language', fontsize=15)
plt.show()

print(classification_report(y_test, y_pred))

model.save('language_identifcation_model.h5')

model = load_model('language_identifcation_model.h5')

sent = """आप कितना सोचते हो
अगर आप ठिठुरती रातों को गिनें
अरे क्या आप मिल सकते हैं (अरे, क्या आप मिल सकते हैं?)
क्या तुम मिलोगे (क्या तुम मिलोगे?)
सर्दियों का अंत बताओ
एक कोमल वसंत के दिन तक
मैं चाहता हूं कि तुम तब तक रहो जब तक फूल खिल न जाएं
ज्यों का त्यों"""


sent = cv.transform([sent])
ans = model.predict(sent)
ans = np.argmax(ans)
le.inverse_transform([ans])
